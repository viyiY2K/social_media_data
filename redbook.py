import pandas as pd
from datetime import datetime
import requests
import json
import time
import os
import subprocess
import sys
import asyncio
import logging

"""
ä»å°çº¢ä¹¦åˆ›ä½œè€…ä¸­å¿ƒå¯¼å‡ºæ•°æ®ï¼Œæœ¬åœ°å¤‡ä»½å¹¶å¢é‡æ›´æ–°åˆ°é£ä¹¦è¡¨æ ¼
"""

# é£ä¹¦é…ç½®
FEISHU_APP_ID = "your_app_id"          # é£ä¹¦åº”ç”¨ID
FEISHU_APP_SECRET = "your_app_secret"  # é£ä¹¦åº”ç”¨å¯†é’¥
FEISHU_APP_TOKEN = "your_app_token"    # é£ä¹¦åº”ç”¨ä»¤ç‰Œ
FEISHU_TABLE_ID = "your_table_id"      # é£ä¹¦å¤šç»´è¡¨æ ¼å­è¡¨ID

# æ•°æ®æ–‡ä»¶é…ç½®
DATA_CSV_PATH = os.path.join("data", "redbook_data.csv")
EXCEL_DIR = os.path.join("downloads", "redbook")

# é…ç½®æ—¥å¿—
def setup_logging():
    """è®¾ç½®æ—¥å¿—é…ç½®"""
    # ç¡®ä¿logsç›®å½•å­˜åœ¨
    log_dir = "logs"
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)
    
    # ç”Ÿæˆæ—¥å¿—æ–‡ä»¶å
    log_filename = f"redbook_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    log_path = os.path.join(log_dir, log_filename)
    
    # é…ç½®æ—¥å¿—æ ¼å¼
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_path, encoding='utf-8'),
            logging.StreamHandler()  # åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°
        ]
    )
    
    return log_path

def get_feishu_access_token():
    """è·å–é£ä¹¦è®¿é—®ä»¤ç‰Œ"""
    url = "https://open.feishu.cn/open-apis/auth/v3/tenant_access_token/internal"
    payload = {"app_id": FEISHU_APP_ID, "app_secret": FEISHU_APP_SECRET}
    
    try:
        response = requests.post(url, json=payload, headers={'Content-Type': 'application/json'})
        result = response.json()
        
        if result.get('code') == 0:
            logging.info("âœ… æˆåŠŸè·å–é£ä¹¦è®¿é—®ä»¤ç‰Œ")
            return result.get('tenant_access_token')
        else:
            logging.error(f"âŒ è·å–é£ä¹¦è®¿é—®ä»¤ç‰Œå¤±è´¥: {result.get('msg')}")
            return None
    except Exception as e:
        logging.error(f"âŒ è·å–é£ä¹¦è®¿é—®ä»¤ç‰Œå¼‚å¸¸: {e}")
        return None

def write_to_feishu_table(data_list, access_token, table_id, columns):
    """å†™å…¥æ•°æ®åˆ°é£ä¹¦æ•°æ®è¡¨ï¼ˆæ”¯æŒåˆ†æ‰¹å†™å…¥ï¼‰"""
    if not data_list or not access_token or not table_id:
        print("âŒ æ•°æ®ä¸ºç©ºæˆ–å‚æ•°æ— æ•ˆï¼Œè·³è¿‡é£ä¹¦å†™å…¥")
        return False
    
    url = f"https://open.feishu.cn/open-apis/bitable/v1/apps/{FEISHU_APP_TOKEN}/tables/{table_id}/records/batch_create"
    headers = {'Content-Type': 'application/json', 'Authorization': f'Bearer {access_token}'}
    
    # åˆ†æ‰¹å¤„ç†ï¼Œæ¯æ‰¹æœ€å¤š500æ¡
    batch_size = 500
    total_records = len(data_list)
    success_count = 0
    
    for i in range(0, total_records, batch_size):
        batch_data = data_list[i:i + batch_size]
        batch_num = i // batch_size + 1
        total_batches = (total_records + batch_size - 1) // batch_size
        
        print(f"ğŸ“ æ­£åœ¨å†™å…¥ç¬¬ {batch_num}/{total_batches} æ‰¹æ•°æ® ({len(batch_data)} æ¡è®°å½•)...")
        
        # è½¬æ¢æ•°æ®æ ¼å¼
        records = []
        for item in batch_data:
            fields = {}
            for col in columns:
                value = item.get(col, '')
                
                # å¤„ç†æ—¶é—´å­—æ®µ
                if col == 'é¦–æ¬¡å‘å¸ƒæ—¶é—´':
                    if pd.notna(value) and value != '':
                        try:
                            if isinstance(value, str):
                                # å¤„ç†ä¸­æ–‡æ—¶é—´æ ¼å¼ï¼š2025å¹´07æœˆ20æ—¥17æ—¶37åˆ†34ç§’
                                if 'å¹´' in value and 'æœˆ' in value and 'æ—¥' in value:
                                    # æ›¿æ¢ä¸­æ–‡å­—ç¬¦ä¸ºæ ‡å‡†æ ¼å¼
                                    time_str = value.replace('å¹´', '-').replace('æœˆ', '-').replace('æ—¥', ' ')
                                    time_str = time_str.replace('æ—¶', ':').replace('åˆ†', ':').replace('ç§’', '')
                                    # åŸå§‹æ—¶é—´æ˜¯ä¸­å›½æ—¶é—´ï¼Œéœ€è¦è½¬æ¢ä¸ºUTC
                                    time_obj = pd.to_datetime(time_str)
                                    # å‡å»8å°æ—¶è½¬æ¢ä¸ºUTCæ—¶é—´
                                    time_obj_utc = time_obj - pd.Timedelta(hours=8)
                                    fields[col] = int(time_obj_utc.timestamp() * 1000)
                                else:
                                    # å°è¯•ç›´æ¥è§£æï¼Œå‡è®¾ä¹Ÿæ˜¯ä¸­å›½æ—¶é—´
                                    time_obj = pd.to_datetime(value)
                                    time_obj_utc = time_obj - pd.Timedelta(hours=8)
                                    fields[col] = int(time_obj_utc.timestamp() * 1000)
                            else:
                                # å‡è®¾æ˜¯ä¸­å›½æ—¶é—´ï¼Œè½¬æ¢ä¸ºUTC
                                time_obj = pd.to_datetime(value)
                                time_obj_utc = time_obj - pd.Timedelta(hours=8)
                                fields[col] = int(time_obj_utc.timestamp() * 1000)
                        except Exception as e:
                            print(f"âš ï¸ æ—¶é—´å­—æ®µ '{col}' å€¼ '{value}' è½¬æ¢å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å­—ç¬¦ä¸²: {e}")
                            fields[col] = str(value)
                    else:
                        fields[col] = ''
                # å¤„ç†æ–‡æœ¬å­—æ®µ
                elif col in ['ç¬”è®°æ ‡é¢˜', 'ä½“è£']:
                    fields[col] = str(value)[:1000] if pd.notna(value) else ''  # é™åˆ¶é•¿åº¦é¿å…è¿‡é•¿
                # å¤„ç†æ•°å­—å­—æ®µï¼ˆå…¶ä»–æ‰€æœ‰å­—æ®µï¼‰
                else:
                    try:
                        fields[col] = int(float(value)) if pd.notna(value) and value != '' else 0
                    except:
                        fields[col] = 0
            
            records.append({"fields": fields})
        
        try:
            response = requests.post(url, json={"records": records}, headers=headers)
            result = response.json()
            
            if result.get('code') == 0:
                success_count += len(batch_data)
                print(f"âœ… ç¬¬ {batch_num} æ‰¹æ•°æ®å†™å…¥æˆåŠŸï¼")
            else:
                print(f"âŒ ç¬¬ {batch_num} æ‰¹æ•°æ®å†™å…¥å¤±è´¥: {result.get('msg')}")
                return False
                
        except Exception as e:
            print(f"âŒ ç¬¬ {batch_num} æ‰¹æ•°æ®å†™å…¥å¼‚å¸¸: {e}")
            return False
        
        # æ·»åŠ å»¶è¿Ÿé¿å…è§¦å‘é¢‘ç‡é™åˆ¶ï¼ˆ10æ¬¡/ç§’ï¼‰
        if batch_num < total_batches:
            time.sleep(0.2)  # 200mså»¶è¿Ÿ
    
    print(f"ğŸ‰ æ‰€æœ‰æ•°æ®å†™å…¥å®Œæˆï¼æ€»å…±æˆåŠŸå†™å…¥ {success_count} æ¡è®°å½•")
    return True

def read_excel_data(file_path):
    """è¯»å–Excelæ–‡ä»¶æ•°æ®ï¼ˆä»ç¬¬äºŒè¡Œå¼€å§‹ï¼Œç¬¬äºŒè¡Œä½œä¸ºè¡¨å¤´ï¼‰"""
    try:
        print(f"ğŸ“– æ­£åœ¨è¯»å–Excelæ–‡ä»¶: {file_path}")
        
        # è¯»å–Excelæ–‡ä»¶ï¼Œè·³è¿‡ç¬¬ä¸€è¡Œï¼Œç¬¬äºŒè¡Œä½œä¸ºè¡¨å¤´
        df = pd.read_excel(file_path, header=1)  # header=1è¡¨ç¤ºç¬¬äºŒè¡Œä½œä¸ºè¡¨å¤´
        
        # åˆ é™¤æ‰€æœ‰åˆ—éƒ½ä¸ºç©ºçš„è¡Œ
        df = df.dropna(how='all')
        
        print(f"ğŸ“Š è¯»å–åˆ° {len(df)} è¡Œæ•°æ®ï¼Œ{len(df.columns)} åˆ—")
        print(f"ğŸ“‹ åˆ—å: {list(df.columns)}")
        
        # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
        data_list = df.to_dict('records')
        
        return data_list, list(df.columns)
        
    except Exception as e:
        print(f"âŒ è¯»å–Excelæ–‡ä»¶å¤±è´¥: {e}")
        return [], []

def merge_data_with_history(excel_data, existing_csv_path, unique_key='é¦–æ¬¡å‘å¸ƒæ—¶é—´'):
    """å°†Excelæ•°æ®ä¸ç°æœ‰CSVæ•°æ®åˆå¹¶ï¼Œä½¿ç”¨å‘å¸ƒæ—¶é—´ä½œä¸ºå”¯ä¸€æ ‡è¯†ç¬¦"""
    # è¯»å–ç°æœ‰CSVæ•°æ®
    if os.path.exists(existing_csv_path):
        try:
            existing_df = pd.read_csv(existing_csv_path)
            print(f"ğŸ“– è¯»å–åˆ°ç°æœ‰CSVæ•°æ®: {len(existing_df)} æ¡è®°å½•")
        except Exception as e:
            print(f"âš ï¸ è¯»å–ç°æœ‰CSVæ–‡ä»¶å¤±è´¥: {e}ï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶")
            existing_df = pd.DataFrame()
    else:
        existing_df = pd.DataFrame()
        print("ğŸ“„ æœªæ‰¾åˆ°ç°æœ‰CSVæ–‡ä»¶ï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶")
    
    # å°†Excelæ•°æ®è½¬æ¢ä¸ºDataFrame
    excel_df = pd.DataFrame(excel_data)
    
    if existing_df.empty:
        # å¦‚æœæ²¡æœ‰å†å²æ•°æ®ï¼Œç›´æ¥ä½¿ç”¨Excelæ•°æ®
        merged_df = excel_df
        new_records = len(excel_df)
        updated_records = 0
    else:
        # ç¡®ä¿ä¸¤ä¸ªDataFrameéƒ½æœ‰å”¯ä¸€æ ‡è¯†ç¬¦åˆ—
        if unique_key not in existing_df.columns:
            print(f"âš ï¸ ç°æœ‰CSVæ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°å”¯ä¸€æ ‡è¯†ç¬¦åˆ— '{unique_key}'ï¼Œå°†ç›´æ¥è¿½åŠ Excelæ•°æ®")
            merged_df = pd.concat([existing_df, excel_df], ignore_index=True)
            new_records = len(excel_df)
            updated_records = 0
        elif unique_key not in excel_df.columns:
            print(f"âš ï¸ Excelæ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°å”¯ä¸€æ ‡è¯†ç¬¦åˆ— '{unique_key}'ï¼Œå°†ç›´æ¥è¿½åŠ Excelæ•°æ®")
            merged_df = pd.concat([existing_df, excel_df], ignore_index=True)
            new_records = len(excel_df)
            updated_records = 0
        else:
            # çœŸæ­£æ¯”è¾ƒæ•°æ®å†…å®¹çš„å˜åŒ–
            excel_keys = set(excel_df[unique_key].astype(str))
            existing_keys = set(existing_df[unique_key].astype(str))
            
            # æ–°å¢çš„è®°å½•
            new_keys = excel_keys - existing_keys
            new_records = len(new_keys)
            
            # æ£€æŸ¥çœŸæ­£æœ‰å†…å®¹å˜åŒ–çš„è®°å½•
            updated_records = 0
            actually_updated_keys = set()
            
            # ä¸ºç°æœ‰æ•°æ®å»ºç«‹ç´¢å¼•ï¼Œæ–¹ä¾¿æŸ¥æ‰¾
            existing_dict = {}
            for _, row in existing_df.iterrows():
                key = str(row[unique_key])
                existing_dict[key] = row.to_dict()
            
            # é€æ¡æ¯”è¾ƒExcelæ•°æ®ä¸ç°æœ‰æ•°æ®
            for _, excel_row in excel_df.iterrows():
                excel_key = str(excel_row[unique_key])
                
                if excel_key in existing_dict:
                    # è®°å½•å­˜åœ¨ï¼Œæ¯”è¾ƒå†…å®¹æ˜¯å¦æœ‰å˜åŒ–
                    existing_row = existing_dict[excel_key]
                    has_changes = False
                    
                    # æ¯”è¾ƒæ¯ä¸ªå­—æ®µï¼ˆé™¤äº†æ—¶é—´å­—æ®µï¼‰
                    for col in excel_df.columns:
                        if col == unique_key:
                            continue  # è·³è¿‡ä¸»é”®å­—æ®µ
                        
                        excel_value = excel_row[col]
                        existing_value = existing_row.get(col, '')
                        
                        # å¤„ç†NaNå€¼
                        if pd.isna(excel_value):
                            excel_value = ''
                        if pd.isna(existing_value):
                            existing_value = ''
                        
                        # å¯¹äºæ•°å­—å­—æ®µï¼Œè½¬æ¢ä¸ºæ•°å­—æ¯”è¾ƒ
                        if col not in ['ç¬”è®°æ ‡é¢˜', 'ä½“è£', 'é¦–æ¬¡å‘å¸ƒæ—¶é—´']:
                            try:
                                excel_num = float(excel_value) if excel_value != '' else 0
                                existing_num = float(existing_value) if existing_value != '' else 0
                                if excel_num != existing_num:
                                    has_changes = True
                                    break
                            except:
                                # å¦‚æœè½¬æ¢å¤±è´¥ï¼ŒæŒ‰å­—ç¬¦ä¸²æ¯”è¾ƒ
                                if str(excel_value) != str(existing_value):
                                    has_changes = True
                                    break
                        else:
                            # æ–‡æœ¬å­—æ®µç›´æ¥æ¯”è¾ƒ
                            if str(excel_value) != str(existing_value):
                                has_changes = True
                                break
                    
                    if has_changes:
                        updated_records += 1
                        actually_updated_keys.add(excel_key)
            
            # åˆå¹¶æ•°æ®ï¼šExcelæ•°æ®ä¼˜å…ˆï¼ˆæ›´æ–°ç°æœ‰è®°å½•ï¼‰
            merged_df = pd.concat([existing_df, excel_df]).drop_duplicates(
                subset=[unique_key], keep='last'
            ).reset_index(drop=True)
            
            # è¾“å‡ºè¯¦ç»†çš„å˜åŒ–ä¿¡æ¯
            if updated_records > 0:
                print(f"ğŸ” æ£€æµ‹åˆ°æ•°æ®å˜åŒ–çš„è®°å½•:")
                for key in actually_updated_keys:
                    print(f"  - {key}")
    
    print(f"ğŸ“Š åˆå¹¶åæ•°æ®: {len(merged_df)} æ¡è®°å½•")
    print(f"ğŸ“ˆ æ–°å¢è®°å½•: {new_records} æ¡")
    print(f"ğŸ”„ çœŸæ­£æ›´æ–°è®°å½•: {updated_records} æ¡")
    
    return merged_df.to_dict('records')

def get_feishu_tables(access_token):
    """è·å–é£ä¹¦å¤šç»´è¡¨æ ¼ä¸­çš„æ‰€æœ‰æ•°æ®è¡¨"""
    url = f"https://open.feishu.cn/open-apis/bitable/v1/apps/{FEISHU_APP_TOKEN}/tables"
    headers = {'Content-Type': 'application/json', 'Authorization': f'Bearer {access_token}'}
    
    try:
        print("ğŸ“‹ æ­£åœ¨è·å–ç°æœ‰è¡¨æ ¼åˆ—è¡¨...")
        response = requests.get(url, headers=headers)
        result = response.json()
        
        if result.get('code') == 0:
            tables = result.get('data', {}).get('items', [])
            print(f"ğŸ“Š æ‰¾åˆ° {len(tables)} ä¸ªç°æœ‰è¡¨æ ¼")
            return tables
        else:
            print(f"âŒ è·å–è¡¨æ ¼åˆ—è¡¨å¤±è´¥: {result.get('msg')}")
            return []
    except Exception as e:
        print(f"âŒ è·å–è¡¨æ ¼åˆ—è¡¨å¼‚å¸¸: {e}")
        return []

def find_target_table(access_token, target_name="å°çº¢ä¹¦"):
    """æŸ¥æ‰¾ç›®æ ‡è¡¨æ ¼"""
    tables = get_feishu_tables(access_token)
    
    for table in tables:
        if table.get('name') == target_name:
            table_id = table.get('table_id')
            print(f"âœ… æ‰¾åˆ°ç›®æ ‡è¡¨æ ¼: {target_name} (ID: {table_id})")
            return table_id
    
    print(f"âš ï¸ æœªæ‰¾åˆ°åä¸º '{target_name}' çš„è¡¨æ ¼")
    return None

def save_data_to_csv(data_list, csv_path):
    """ä¿å­˜æ•°æ®åˆ°CSVæ–‡ä»¶"""
    if not data_list:
        print("âŒ æ²¡æœ‰æ•°æ®å¯ä¿å­˜åˆ°CSV")
        return None
    
    try:
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(csv_path), exist_ok=True)
        
        df = pd.DataFrame(data_list)
        df.to_csv(csv_path, index=False, encoding='utf-8-sig')
        print(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ°æœ¬åœ°CSVæ–‡ä»¶: {csv_path}")
        return csv_path
    except Exception as e:
        print(f"âŒ ä¿å­˜CSVæ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return None

def find_latest_excel_file(excel_dir, max_age_hours=24):
    """æŸ¥æ‰¾æœ€æ–°çš„Excelæ–‡ä»¶ï¼Œå¹¶æ£€æŸ¥æ–‡ä»¶æ—¶é—´"""
    if not os.path.exists(excel_dir):
        logging.error(f"âŒ Excelç›®å½•ä¸å­˜åœ¨: {excel_dir}")
        return None
    
    excel_files = []
    current_time = time.time()
    
    for f in os.listdir(excel_dir):
        if f.endswith('.xlsx') or f.endswith('.xls'):
            file_path = os.path.join(excel_dir, f)
            file_mtime = os.path.getmtime(file_path)
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦åœ¨æŒ‡å®šæ—¶é—´å†…åˆ›å»º
            age_hours = (current_time - file_mtime) / 3600
            
            if age_hours <= max_age_hours:
                excel_files.append((file_path, file_mtime, age_hours))
                logging.info(f"âœ… æ‰¾åˆ°æœ‰æ•ˆæ–‡ä»¶: {f} (åˆ›å»ºäº {age_hours:.1f} å°æ—¶å‰)")
            else:
                logging.warning(f"âš ï¸ æ–‡ä»¶è¿‡æ—§ï¼Œè·³è¿‡: {f} (åˆ›å»ºäº {age_hours:.1f} å°æ—¶å‰)")
    
    if not excel_files:
        logging.error(f"âŒ åœ¨ {excel_dir} ç›®å½•ä¸­æœªæ‰¾åˆ°{max_age_hours}å°æ—¶å†…çš„Excelæ–‡ä»¶")
        return None
    
    # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œè¿”å›æœ€æ–°çš„æ–‡ä»¶
    excel_files.sort(key=lambda x: x[1], reverse=True)
    latest_file = excel_files[0][0]
    latest_age = excel_files[0][2]
    
    logging.info(f"ğŸ“ é€‰æ‹©æœ€æ–°Excelæ–‡ä»¶: {os.path.basename(latest_file)} (åˆ›å»ºäº {latest_age:.1f} å°æ—¶å‰)")
    return latest_file

async def run_redbook_data_export():
    """è¿è¡Œredbook_data.pyå¯¼å‡ºæœ€æ–°æ•°æ®"""
    try:
        logging.info("ğŸš€ å¼€å§‹è¿è¡Œæ•°æ®å¯¼å‡ºè„šæœ¬...")
        
        # è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•
        current_dir = os.path.dirname(os.path.abspath(__file__))
        redbook_data_script = os.path.join(current_dir, "redbook_data.py")
        
        if not os.path.exists(redbook_data_script):
            logging.error(f"âŒ æœªæ‰¾åˆ°æ•°æ®å¯¼å‡ºè„šæœ¬: {redbook_data_script}")
            return False
        
        logging.info(f"ğŸ“‚ æ‰§è¡Œè„šæœ¬: {redbook_data_script}")
        
        # ä½¿ç”¨subprocessè¿è¡Œredbook_data.py
        process = subprocess.Popen(
            [sys.executable, redbook_data_script],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            cwd=current_dir
        )
        
        # å®æ—¶è¾“å‡ºæ—¥å¿—
        while True:
            output = process.stdout.readline()
            if output == '' and process.poll() is not None:
                break
            if output:
                logging.info(f"[SUBPROCESS] {output.strip()}")
        
        # è·å–è¿”å›ç 
        return_code = process.poll()
        
        if return_code == 0:
            logging.info("âœ… æ•°æ®å¯¼å‡ºè„šæœ¬æ‰§è¡ŒæˆåŠŸï¼")
            return True
        else:
            stderr_output = process.stderr.read()
            logging.error(f"âŒ æ•°æ®å¯¼å‡ºè„šæœ¬æ‰§è¡Œå¤±è´¥ï¼Œè¿”å›ç : {return_code}")
            if stderr_output:
                logging.error(f"é”™è¯¯ä¿¡æ¯: {stderr_output}")
            return False
            
    except Exception as e:
        logging.error(f"âŒ è¿è¡Œæ•°æ®å¯¼å‡ºè„šæœ¬æ—¶å‡ºé”™: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    # è®¾ç½®æ—¥å¿—
    log_path = setup_logging()
    logging.info("ğŸ” å¼€å§‹å¤„ç†å°çº¢ä¹¦æ•°æ®...")
    logging.info(f"ğŸ“ æ—¥å¿—æ–‡ä»¶: {log_path}")
    
    try:
        # é¦–å…ˆè¿è¡Œæ•°æ®å¯¼å‡ºè„šæœ¬
        logging.info("\nğŸ“¥ ===== ç¬¬ä¸€æ­¥ï¼šå¯¼å‡ºæœ€æ–°æ•°æ® =====")
        export_success = False
        export_error = None
        
        try:
            # è¿è¡Œå¼‚æ­¥å‡½æ•°
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            export_success = loop.run_until_complete(run_redbook_data_export())
            loop.close()
            
            if not export_success:
                logging.warning("âš ï¸ æ•°æ®å¯¼å‡ºå¤±è´¥ï¼Œä½†ç»§ç»­å¤„ç†ç°æœ‰æ•°æ®...")
                export_error = "æ•°æ®å¯¼å‡ºè¿”å›å¤±è´¥çŠ¶æ€"
            else:
                logging.info("âœ… æ•°æ®å¯¼å‡ºå®Œæˆï¼")
                time.sleep(2)
        except Exception as e:
            logging.error(f"âŒ æ•°æ®å¯¼å‡ºè¿‡ç¨‹å‡ºé”™: {e}")
            logging.warning("âš ï¸ ç»§ç»­å¤„ç†ç°æœ‰æ•°æ®...")
            export_error = str(e)
        
        logging.info("\nğŸ“Š ===== ç¬¬äºŒæ­¥ï¼šå¤„ç†å’Œä¸Šä¼ æ•°æ® =====")
        
        # æŸ¥æ‰¾æœ€æ–°çš„Excelæ–‡ä»¶
        excel_file = find_latest_excel_file(EXCEL_DIR)
        if not excel_file:
            logging.error("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„Excelæ–‡ä»¶")
            if export_error:
                print(f"STATUS:DATA_EXPORT_FAILED_AND_NO_RECENT_EXCEL:{export_error}")
                sys.exit(9)  # æ•°æ®å¯¼å‡ºå¤±è´¥ä¸”æ— æœ€è¿‘çš„Excelæ–‡ä»¶
            else:
                print("STATUS:NO_RECENT_EXCEL_FILE")
                sys.exit(10)  # æ— æœ€è¿‘çš„Excelæ–‡ä»¶
        
        # è¯»å–Excelæ•°æ®
        excel_data, columns = read_excel_data(excel_file)
        
        if not excel_data:
            logging.error("âŒ æœªè¯»å–åˆ°ä»»ä½•æ•°æ®")
            if export_error:
                print(f"STATUS:DATA_EXPORT_FAILED_AND_NO_DATA:{export_error}")
                sys.exit(8)  # æ•°æ®å¯¼å‡ºå¤±è´¥ä¸”æ— æ•°æ®
            else:
                print("STATUS:NO_DATA")
                sys.exit(4)
        
        # ä¸å†å²æ•°æ®åˆå¹¶
        logging.info("\nğŸ”„ å¼€å§‹åˆå¹¶å†å²æ•°æ®...")
        merged_data = merge_data_with_history(excel_data, DATA_CSV_PATH)
        
        # ä¿å­˜åˆå¹¶åçš„æ•°æ®åˆ°CSVæ–‡ä»¶
        csv_path = save_data_to_csv(merged_data, DATA_CSV_PATH)
        if not csv_path:
            logging.error("âŒ ä¿å­˜CSVæ–‡ä»¶å¤±è´¥")
            print("STATUS:CSV_SAVE_FAILED")
            sys.exit(5)
        
        logging.info(f"\nğŸš€ å¼€å§‹å¢é‡æ›´æ–°é£ä¹¦å¤šç»´è¡¨æ ¼...")
        
        # è·å–é£ä¹¦è®¿é—®ä»¤ç‰Œ
        access_token = get_feishu_access_token()
        if not access_token:
            logging.error("âŒ æ— æ³•è·å–é£ä¹¦è®¿é—®ä»¤ç‰Œï¼Œè·³è¿‡é£ä¹¦æ›´æ–°")
            print("STATUS:FEISHU_TOKEN_FAILED")
            sys.exit(6)
        
        # ä½¿ç”¨å›ºå®šçš„è¡¨æ ¼IDè¿›è¡Œå¢é‡æ›´æ–°
        logging.info(f"ğŸ“‹ ä½¿ç”¨å›ºå®šè¡¨æ ¼ID: {FEISHU_TABLE_ID}")
        success = incremental_update_feishu_table(merged_data, access_token, FEISHU_TABLE_ID, columns)
        
        if success:
            logging.info(f"\nğŸ‰ æ•°æ®å·²æˆåŠŸå¢é‡æ›´æ–°åˆ°é£ä¹¦æ•°æ®è¡¨")
            logging.info(f"ğŸ“‹ å¤„ç†äº† {len(merged_data)} æ¡æ•°æ®")
            logging.info(f"ğŸ’¾ æœ¬åœ°å¤‡ä»½æ–‡ä»¶: {csv_path}")
            logging.info(f"\nğŸ“ˆ æ€»å…±æˆåŠŸå¤„ç†äº† {len(merged_data)} æ¡å°çº¢ä¹¦æ•°æ®")
            logging.info(f"ğŸ“ è¯¦ç»†æ—¥å¿—å·²ä¿å­˜åˆ°: {log_path}")
            
            # è¾“å‡ºæˆåŠŸçŠ¶æ€
            if export_error:
                print("STATUS:SUCCESS_WITH_EXPORT_WARNING")
                print(f"EXPORT_WARNING:{export_error}")
            else:
                print("STATUS:SUCCESS")
            
            print(f"PROCESSED_RECORDS:{len(merged_data)}")
            print(f"CSV_PATH:{csv_path}")
            print(f"LOG_PATH:{log_path}")
            sys.exit(0)  # æˆåŠŸé€€å‡º
        else:
            logging.error("âŒ é£ä¹¦æ•°æ®å¢é‡æ›´æ–°å¤±è´¥")
            if export_error:
                print("STATUS:FEISHU_UPDATE_FAILED_WITH_EXPORT_WARNING")
                print(f"EXPORT_WARNING:{export_error}")
            else:
                print("STATUS:FEISHU_UPDATE_FAILED")
            
            print(f"PROCESSED_RECORDS:{len(merged_data)}")
            print(f"CSV_PATH:{csv_path}")
            sys.exit(1)  # é£ä¹¦æ›´æ–°å¤±è´¥
            
    except KeyboardInterrupt:
        logging.error("âŒ ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
        print("STATUS:USER_INTERRUPTED")
        sys.exit(130)  # ç”¨æˆ·ä¸­æ–­
        
    except Exception as e:
        logging.error(f"âŒ è„šæœ¬è¿è¡Œå¼‚å¸¸: {e}")
        print(f"STATUS:EXCEPTION:{str(e)[:200]}")
        sys.exit(2)  # å¼‚å¸¸é€€å‡º

def get_existing_records(access_token, table_id):
    """è·å–è¡¨æ ¼ä¸­çš„ç°æœ‰è®°å½•"""
    url = f"https://open.feishu.cn/open-apis/bitable/v1/apps/{FEISHU_APP_TOKEN}/tables/{table_id}/records"
    headers = {'Content-Type': 'application/json', 'Authorization': f'Bearer {access_token}'}
    
    all_records = []
    page_token = None
    
    try:
        while True:
            params = {'page_size': 500}
            if page_token:
                params['page_token'] = page_token
            
            response = requests.get(url, headers=headers, params=params)
            result = response.json()
            
            if result.get('code') != 0:
                print(f"âŒ è·å–è®°å½•å¤±è´¥: {result.get('msg')}")
                return {}
            
            records = result.get('data', {}).get('items', [])
            all_records.extend(records)
            
            # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ›´å¤šé¡µ
            page_token = result.get('data', {}).get('page_token')
            if not page_token:
                break
        
        print(f"ğŸ“Š è·å–åˆ° {len(all_records)} æ¡ç°æœ‰è®°å½•")
        
        # æ„å»ºä»¥å‘å¸ƒæ—¶é—´ä¸ºkeyçš„å­—å…¸ï¼Œæ–¹ä¾¿æŸ¥æ‰¾
        records_dict = {}
        for record in all_records:
            fields = record.get('fields', {})
            publish_time = fields.get('é¦–æ¬¡å‘å¸ƒæ—¶é—´', '')
            if publish_time:
                # å°†æ—¶é—´æˆ³è½¬æ¢ä¸ºå­—ç¬¦ä¸²ç”¨äºåŒ¹é…
                if isinstance(publish_time, (int, float)):
                    # é£ä¹¦å­˜å‚¨çš„æ˜¯UTCæ—¶é—´æˆ³ï¼ˆæ¯«ç§’ï¼‰ï¼Œè½¬æ¢ä¸ºä¸­å›½æ—¶é—´å­—ç¬¦ä¸²
                    import datetime
                    utc_time = datetime.datetime.fromtimestamp(publish_time / 1000, tz=datetime.timezone.utc)
                    china_time = utc_time + datetime.timedelta(hours=8)
                    time_str = china_time.strftime('%Yå¹´%mæœˆ%dæ—¥%Hæ—¶%Måˆ†%Sç§’')
                else:
                    time_str = str(publish_time)
                
                records_dict[time_str] = {
                    'record_id': record['record_id'],
                    'fields': fields
                }
        
        return records_dict
        
    except Exception as e:
        print(f"âŒ è·å–ç°æœ‰è®°å½•å¼‚å¸¸: {e}")
        return {}

def compare_and_prepare_updates(new_data, existing_records, columns):
    """æ¯”è¾ƒæ–°æ—§æ•°æ®ï¼Œå‡†å¤‡æ›´æ–°ã€æ–°å¢å’Œåˆ é™¤çš„è®°å½•"""
    updates = []  # éœ€è¦æ›´æ–°çš„è®°å½•
    creates = []  # éœ€è¦æ–°å¢çš„è®°å½•
    
    print("ğŸ” å¼€å§‹æ¯”è¾ƒæ•°æ®å˜åŒ–...")
    
    for item in new_data:
        publish_time = item.get('é¦–æ¬¡å‘å¸ƒæ—¶é—´', '')
        if not publish_time:
            continue
        
        # æ ‡å‡†åŒ–æ—¶é—´æ ¼å¼ç”¨äºåŒ¹é…
        if isinstance(publish_time, str) and ('å¹´' in publish_time):
            time_key = publish_time
        else:
            # å¦‚æœæ˜¯å…¶ä»–æ ¼å¼ï¼Œå°è¯•è½¬æ¢
            try:
                if isinstance(publish_time, str):
                    time_obj = pd.to_datetime(publish_time)
                else:
                    time_obj = pd.to_datetime(publish_time)
                time_key = time_obj.strftime('%Yå¹´%mæœˆ%dæ—¥%Hæ—¶%Måˆ†%Sç§’')
            except:
                time_key = str(publish_time)
        
        if time_key in existing_records:
            # è®°å½•å­˜åœ¨ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
            existing_record = existing_records[time_key]
            existing_fields = existing_record['fields']
            record_id = existing_record['record_id']
            
            # æ¯”è¾ƒå„å­—æ®µæ˜¯å¦æœ‰å˜åŒ–
            has_changes = False
            updated_fields = {}
            
            for col in columns:
                new_value = item.get(col, '')
                
                # å¤„ç†ä¸åŒç±»å‹çš„å­—æ®µ
                if col == 'é¦–æ¬¡å‘å¸ƒæ—¶é—´':
                    # æ—¶é—´å­—æ®µä¸éœ€è¦æ›´æ–°ï¼ˆä½œä¸ºä¸»é”®ï¼‰
                    continue
                elif col in ['ç¬”è®°æ ‡é¢˜', 'ä½“è£']:
                    # æ–‡æœ¬å­—æ®µ
                    new_str = str(new_value)[:1000] if pd.notna(new_value) else ''
                    existing_str = str(existing_fields.get(col, ''))
                    if new_str != existing_str:
                        updated_fields[col] = new_str
                        has_changes = True
                else:
                    # æ•°å­—å­—æ®µ
                    try:
                        new_num = int(float(new_value)) if pd.notna(new_value) and new_value != '' else 0
                    except:
                        new_num = 0
                    
                    existing_num = existing_fields.get(col, 0)
                    if isinstance(existing_num, str):
                        try:
                            existing_num = int(float(existing_num))
                        except:
                            existing_num = 0
                    
                    if new_num != existing_num:
                        updated_fields[col] = new_num
                        has_changes = True
            
            if has_changes:
                updates.append({
                    'record_id': record_id,
                    'fields': updated_fields
                })
        else:
            # æ–°è®°å½•ï¼Œéœ€è¦åˆ›å»º
            fields = {}
            for col in columns:
                value = item.get(col, '')
                
                # å¤„ç†æ—¶é—´å­—æ®µ
                if col == 'é¦–æ¬¡å‘å¸ƒæ—¶é—´':
                    if pd.notna(value) and value != '':
                        try:
                            if isinstance(value, str) and 'å¹´' in value:
                                time_str = value.replace('å¹´', '-').replace('æœˆ', '-').replace('æ—¥', ' ')
                                time_str = time_str.replace('æ—¶', ':').replace('åˆ†', ':').replace('ç§’', '')
                                time_obj = pd.to_datetime(time_str)
                                time_obj_utc = time_obj - pd.Timedelta(hours=8)
                                fields[col] = int(time_obj_utc.timestamp() * 1000)
                            else:
                                time_obj = pd.to_datetime(value)
                                time_obj_utc = time_obj - pd.Timedelta(hours=8)
                                fields[col] = int(time_obj_utc.timestamp() * 1000)
                        except Exception as e:
                            print(f"âš ï¸ æ—¶é—´å­—æ®µè½¬æ¢å¤±è´¥: {e}")
                            fields[col] = str(value)
                    else:
                        fields[col] = ''
                elif col in ['ç¬”è®°æ ‡é¢˜', 'ä½“è£']:
                    fields[col] = str(value)[:1000] if pd.notna(value) else ''
                else:
                    try:
                        fields[col] = int(float(value)) if pd.notna(value) and value != '' else 0
                    except:
                        fields[col] = 0
            
            creates.append({'fields': fields})
    
    print(f"ğŸ“Š æ•°æ®æ¯”è¾ƒå®Œæˆ:")
    print(f"ğŸ”„ éœ€è¦æ›´æ–°: {len(updates)} æ¡è®°å½•")
    print(f"ğŸ“ éœ€è¦æ–°å¢: {len(creates)} æ¡è®°å½•")
    
    return updates, creates

def batch_update_records(access_token, table_id, updates):
    """æ‰¹é‡æ›´æ–°è®°å½•"""
    if not updates:
        print("ğŸ“„ æ²¡æœ‰éœ€è¦æ›´æ–°çš„è®°å½•")
        return True
    
    url = f"https://open.feishu.cn/open-apis/bitable/v1/apps/{FEISHU_APP_TOKEN}/tables/{table_id}/records/batch_update"
    headers = {'Content-Type': 'application/json', 'Authorization': f'Bearer {access_token}'}
    
    # åˆ†æ‰¹å¤„ç†
    batch_size = 500
    success_count = 0
    
    for i in range(0, len(updates), batch_size):
        batch_updates = updates[i:i + batch_size]
        batch_num = i // batch_size + 1
        total_batches = (len(updates) + batch_size - 1) // batch_size
        
        print(f"ğŸ”„ æ­£åœ¨æ›´æ–°ç¬¬ {batch_num}/{total_batches} æ‰¹æ•°æ® ({len(batch_updates)} æ¡è®°å½•)...")
        
        try:
            response = requests.post(url, json={"records": batch_updates}, headers=headers)
            result = response.json()
            
            if result.get('code') == 0:
                success_count += len(batch_updates)
                print(f"âœ… ç¬¬ {batch_num} æ‰¹æ›´æ–°æˆåŠŸï¼")
            else:
                print(f"âŒ ç¬¬ {batch_num} æ‰¹æ›´æ–°å¤±è´¥: {result.get('msg')}")
                return False
                
        except Exception as e:
            print(f"âŒ ç¬¬ {batch_num} æ‰¹æ›´æ–°å¼‚å¸¸: {e}")
            return False
        
        # æ·»åŠ å»¶è¿Ÿé¿å…é¢‘ç‡é™åˆ¶
        if batch_num < total_batches:
            time.sleep(0.2)
    
    print(f"âœ… æ‰¹é‡æ›´æ–°å®Œæˆï¼ŒæˆåŠŸæ›´æ–° {success_count} æ¡è®°å½•")
    return True

def batch_create_records(access_token, table_id, creates):
    """æ‰¹é‡åˆ›å»ºè®°å½•"""
    if not creates:
        print("ğŸ“„ æ²¡æœ‰éœ€è¦æ–°å¢çš„è®°å½•")
        return True
    
    url = f"https://open.feishu.cn/open-apis/bitable/v1/apps/{FEISHU_APP_TOKEN}/tables/{table_id}/records/batch_create"
    headers = {'Content-Type': 'application/json', 'Authorization': f'Bearer {access_token}'}
    
    # åˆ†æ‰¹å¤„ç†
    batch_size = 500
    success_count = 0
    
    for i in range(0, len(creates), batch_size):
        batch_creates = creates[i:i + batch_size]
        batch_num = i // batch_size + 1
        total_batches = (len(creates) + batch_size - 1) // batch_size
        
        print(f"ğŸ“ æ­£åœ¨æ–°å¢ç¬¬ {batch_num}/{total_batches} æ‰¹æ•°æ® ({len(batch_creates)} æ¡è®°å½•)...")
        
        try:
            response = requests.post(url, json={"records": batch_creates}, headers=headers)
            result = response.json()
            
            if result.get('code') == 0:
                success_count += len(batch_creates)
                print(f"âœ… ç¬¬ {batch_num} æ‰¹æ–°å¢æˆåŠŸï¼")
            else:
                print(f"âŒ ç¬¬ {batch_num} æ‰¹æ–°å¢å¤±è´¥: {result.get('msg')}")
                return False
                
        except Exception as e:
            print(f"âŒ ç¬¬ {batch_num} æ‰¹æ–°å¢å¼‚å¸¸: {e}")
            return False
        
        # æ·»åŠ å»¶è¿Ÿé¿å…é¢‘ç‡é™åˆ¶
        if batch_num < total_batches:
            time.sleep(0.2)
    
    print(f"âœ… æ‰¹é‡æ–°å¢å®Œæˆï¼ŒæˆåŠŸæ–°å¢ {success_count} æ¡è®°å½•")
    return True

def incremental_update_feishu_table(data_list, access_token, table_id, columns):
    """å¢é‡æ›´æ–°é£ä¹¦æ•°æ®è¡¨ï¼ˆåªæ›´æ–°å˜åŒ–çš„æ•°æ®ï¼‰"""
    if not data_list or not access_token or not table_id:
        print("âŒ æ•°æ®ä¸ºç©ºæˆ–å‚æ•°æ— æ•ˆï¼Œè·³è¿‡é£ä¹¦æ›´æ–°")
        return False
    
    print(f"ğŸ”„ å¼€å§‹å¢é‡æ›´æ–°é£ä¹¦è¡¨æ ¼ (ID: {table_id})...")
    
    # ç¬¬ä¸€æ­¥ï¼šè·å–ç°æœ‰è®°å½•
    existing_records = get_existing_records(access_token, table_id)
    
    # ç¬¬äºŒæ­¥ï¼šæ¯”è¾ƒæ•°æ®ï¼Œå‡†å¤‡æ›´æ–°å’Œæ–°å¢åˆ—è¡¨
    updates, creates = compare_and_prepare_updates(data_list, existing_records, columns)
    
    # ç¬¬ä¸‰æ­¥ï¼šæ‰§è¡Œæ›´æ–°
    update_success = batch_update_records(access_token, table_id, updates)
    if not update_success:
        return False
    
    # ç¬¬å››æ­¥ï¼šæ‰§è¡Œæ–°å¢
    create_success = batch_create_records(access_token, table_id, creates)
    if not create_success:
        return False
    
    print(f"ğŸ‰ å¢é‡æ›´æ–°å®Œæˆï¼æ›´æ–°äº† {len(updates)} æ¡è®°å½•ï¼Œæ–°å¢äº† {len(creates)} æ¡è®°å½•")
    return True

if __name__ == "__main__":
    main()