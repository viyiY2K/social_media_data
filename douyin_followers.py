import asyncio
import httpx
import pandas as pd
from datetime import datetime
import json
import re
import sys
import os
import time
import random
from urllib.parse import quote, unquote

class DouyinFansCollectorEnhanced:
    def __init__(self, cookie):
        self.cookie = cookie
        self.session_id = self.extract_session_id(cookie)
        
    def extract_session_id(self, cookie):
        """ä»cookieä¸­æå–sessionid"""
        match = re.search(r'sessionid=([^;]+)', cookie)
        return match.group(1) if match else ''
    
    def get_headers(self, referer="https://www.douyin.com/"):
        """ç”Ÿæˆè¯·æ±‚å¤´"""
        return {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'application/json, text/plain, */*',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Referer': referer,
            'Cookie': self.cookie,
            'Sec-Fetch-Dest': 'empty',
            'Sec-Fetch-Mode': 'cors',
            'Sec-Fetch-Site': 'same-origin',
            'sec-ch-ua': '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
            'sec-ch-ua-mobile': '?0',
            'sec-ch-ua-platform': '"macOS"'
        }
    
    async def get_user_by_search(self, unique_id):
        """é€šè¿‡æœç´¢APIè·å–ç”¨æˆ·ä¿¡æ¯"""
        try:
            print(f"ğŸ” é€šè¿‡æœç´¢è·å–ç”¨æˆ· {unique_id} çš„ä¿¡æ¯...")
            
            # æœç´¢API
            search_url = "https://www.douyin.com/aweme/v1/web/discover/search/"
            params = {
                'device_platform': 'webapp',
                'aid': '6383',
                'channel': 'channel_pc_web',
                'search_channel': 'aweme_user_web',
                'keyword': unique_id,
                'search_source': 'normal_search',
                'query_correct_type': '1',
                'is_filter_search': '0',
                'from_group_id': '',
                'offset': '0',
                'count': '10'
            }
            
            async with httpx.AsyncClient(headers=self.get_headers(), timeout=30) as client:
                response = await client.get(search_url, params=params)
                
                if response.status_code == 200:
                    data = response.json()
                    
                    # è§£ææœç´¢ç»“æœ
                    if 'user_list' in data and data['user_list']:
                        for user in data['user_list']:
                            user_info = user.get('user_info', {})
                            if user_info.get('unique_id') == unique_id or user_info.get('short_id') == unique_id:
                                return self.format_user_data(user_info, unique_id)
                    
                    print(f"âš ï¸ åœ¨æœç´¢ç»“æœä¸­æœªæ‰¾åˆ°ç”¨æˆ· {unique_id}")
                    return None
                else:
                    print(f"âŒ æœç´¢è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                    return None
                    
        except Exception as e:
            print(f"âŒ æœç´¢ç”¨æˆ·ä¿¡æ¯æ—¶å‡ºé”™: {str(e)}")
            return None
    
    async def get_user_by_profile_page(self, unique_id):
        """é€šè¿‡ç”¨æˆ·ä¸»é¡µè·å–ä¿¡æ¯"""
        try:
            print(f"ğŸŒ è®¿é—®ç”¨æˆ· {unique_id} çš„ä¸»é¡µ...")
            
            profile_url = f"https://www.douyin.com/user/{unique_id}"
            
            async with httpx.AsyncClient(headers=self.get_headers(), timeout=30, follow_redirects=True) as client:
                response = await client.get(profile_url)
                
                if response.status_code == 200:
                    html_content = response.text
                    
                    # å°è¯•ä»é¡µé¢ä¸­æå–æ•°æ®
                    user_data = self.extract_from_html(html_content, unique_id)
                    if user_data:
                        return user_data
                    
                    # å°è¯•ä»INITIAL_STATEä¸­æå–
                    user_data = self.extract_from_initial_state(html_content, unique_id)
                    if user_data:
                        return user_data
                    
                    print(f"âš ï¸ æ— æ³•ä»ä¸»é¡µæå–ç”¨æˆ· {unique_id} çš„æ•°æ®")
                    return None
                else:
                    print(f"âŒ è®¿é—®ä¸»é¡µå¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                    return None
                    
        except Exception as e:
            print(f"âŒ è®¿é—®ç”¨æˆ·ä¸»é¡µæ—¶å‡ºé”™: {str(e)}")
            return None
    
    def extract_from_html(self, html_content, unique_id):
        """ä»HTMLä¸­æå–ç”¨æˆ·æ•°æ®"""
        try:
            # æŸ¥æ‰¾ç”¨æˆ·æ•°æ®çš„å¤šç§æ¨¡å¼
            patterns = [
                r'"followerCount":(\d+)',
                r'"user":{[^}]*"followerCount":(\d+)',
                r'"userInfo":{[^}]*"followerCount":(\d+)'
            ]
            
            nickname_patterns = [
                r'"nickname":"([^"]+)"',
                r'"user":{[^}]*"nickname":"([^"]+)"',
                r'"userInfo":{[^}]*"nickname":"([^"]+)"'
            ]
            
            follower_count = 0
            nickname = f"ç”¨æˆ·_{unique_id}"
            
            # æå–ç²‰ä¸æ•°
            for pattern in patterns:
                match = re.search(pattern, html_content)
                if match:
                    follower_count = int(match.group(1))
                    break
            
            # æå–æ˜µç§°
            for pattern in nickname_patterns:
                match = re.search(pattern, html_content)
                if match:
                    nickname = match.group(1)
                    break
            
            if follower_count > 0:
                # ç”Ÿæˆæ—¶é—´æˆ³ä½œä¸ºå”¯ä¸€ç¼–ç 
                timestamp = int(time.time() * 1000)  # æ¯«ç§’çº§æ—¶é—´æˆ³
                
                return {
                    'ç¼–ç ': timestamp,
                    'è´¦å·å': nickname,
                    'å¹³å°': 'æŠ–éŸ³',
                    'ç²‰ä¸æ•°': follower_count,
                    'æ—¥æœŸ': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    'æŠ–éŸ³å·': unique_id  # ä¿ç•™ç”¨äºå†…éƒ¨å¤„ç†ï¼Œä½†ä¸è¾“å‡ºåˆ°CSV
                }
            
            return None
            
        except Exception as e:
            print(f"âš ï¸ HTMLè§£æå‡ºé”™: {str(e)}")
            return None
    
    def extract_from_initial_state(self, html_content, unique_id):
        """ä»INITIAL_STATEä¸­æå–æ•°æ®"""
        try:
            # æŸ¥æ‰¾INITIAL_STATE
            match = re.search(r'window\.__INITIAL_STATE__\s*=\s*({.+?});', html_content)
            if not match:
                return None
            
            try:
                initial_state = json.loads(match.group(1))
                
                # é€’å½’æŸ¥æ‰¾ç”¨æˆ·ä¿¡æ¯
                user_info = self.find_user_in_state(initial_state, unique_id)
                if user_info:
                    return self.format_user_data(user_info, unique_id)
                    
            except json.JSONDecodeError:
                pass
            
            return None
            
        except Exception as e:
            print(f"âš ï¸ INITIAL_STATEè§£æå‡ºé”™: {str(e)}")
            return None
    
    def find_user_in_state(self, data, unique_id, path=""):
        """åœ¨çŠ¶æ€æ•°æ®ä¸­é€’å½’æŸ¥æ‰¾ç”¨æˆ·ä¿¡æ¯"""
        if isinstance(data, dict):
            # æ£€æŸ¥æ˜¯å¦æ˜¯ç”¨æˆ·ä¿¡æ¯
            if ('followerCount' in data or 'follower_count' in data) and \
               ('uniqueId' in data or 'unique_id' in data or 'nickname' in data):
                user_unique_id = data.get('uniqueId') or data.get('unique_id')
                if user_unique_id == unique_id:
                    return data
            
            # é€’å½’æœç´¢
            for key, value in data.items():
                result = self.find_user_in_state(value, unique_id, f"{path}.{key}")
                if result:
                    return result
        
        elif isinstance(data, list):
            for i, item in enumerate(data):
                result = self.find_user_in_state(item, unique_id, f"{path}[{i}]")
                if result:
                    return result
        
        return None
    
    def format_user_data(self, user_info, unique_id):
        """æ ¼å¼åŒ–ç”¨æˆ·æ•°æ®"""
        follower_count = user_info.get('followerCount') or user_info.get('follower_count', 0)
        nickname = user_info.get('nickname') or user_info.get('nick_name') or f"ç”¨æˆ·_{unique_id}"
        
        return {
            'æ—¥æœŸ': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'è´¦å·å': nickname,
            'å¹³å°': 'æŠ–éŸ³',
            'ç²‰ä¸æ•°': follower_count,
            'æŠ–éŸ³å·': unique_id,  # ä¿ç•™ç”¨äºå†…éƒ¨å¤„ç†ï¼Œä½†ä¸è¾“å‡ºåˆ°CSV
            'å…³æ³¨æ•°': user_info.get('followingCount') or user_info.get('following_count', 0),
            'è·èµæ•°': user_info.get('totalFavorited') or user_info.get('total_favorited', 0),
            'ä½œå“æ•°': user_info.get('awemeCount') or user_info.get('aweme_count', 0)
        }
    
    async def get_user_info(self, unique_id):
        """è·å–ç”¨æˆ·ä¿¡æ¯çš„ä¸»æ–¹æ³•"""
        print(f"\nğŸ“Š æ­£åœ¨å¤„ç†æŠ–éŸ³å·: {unique_id}")
        
        # æ–¹æ³•1: é€šè¿‡æœç´¢API
        user_data = await self.get_user_by_search(unique_id)
        if user_data:
            return user_data
        
        # ç­‰å¾…ä¸€æ®µæ—¶é—´
        await asyncio.sleep(2)
        
        # æ–¹æ³•2: é€šè¿‡ç”¨æˆ·ä¸»é¡µ
        user_data = await self.get_user_by_profile_page(unique_id)
        if user_data:
            return user_data
        
        return None
    
    async def collect_fans_data(self, user_list):
        """æ‰¹é‡æ”¶é›†ç²‰ä¸æ•°æ®"""
        all_data = []
        
        for unique_id in user_list:
            if not unique_id or unique_id.strip() == "":
                continue
            
            user_data = await self.get_user_info(unique_id)
            
            if user_data:
                all_data.append(user_data)
                print(f"âœ… æˆåŠŸè·å– {user_data['è´¦å·å']} çš„æ•°æ®")
                print(f"   ç²‰ä¸æ•°: {user_data['ç²‰ä¸æ•°']:,}")
                if 'å…³æ³¨æ•°' in user_data and user_data['å…³æ³¨æ•°'] > 0:
                    print(f"   å…³æ³¨æ•°: {user_data['å…³æ³¨æ•°']:,}")
                if 'ä½œå“æ•°' in user_data and user_data['ä½œå“æ•°'] > 0:
                    print(f"   ä½œå“æ•°: {user_data['ä½œå“æ•°']:,}")
            else:
                print(f"âŒ è·å–æŠ–éŸ³å· {unique_id} çš„æ•°æ®å¤±è´¥")
                all_data.append({
                    'æ—¥æœŸ': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    'è´¦å·å': f"è·å–å¤±è´¥_{unique_id}",
                    'å¹³å°': 'æŠ–éŸ³',
                    'ç²‰ä¸æ•°': 0,
                    'æŠ–éŸ³å·': unique_id,
                    'å¤‡æ³¨': 'æ•°æ®è·å–å¤±è´¥'
                })
            
            # éšæœºå»¶è¿Ÿ
            delay = random.uniform(3, 6)
            print(f"â±ï¸ ç­‰å¾… {delay:.1f} ç§’...")
            await asyncio.sleep(delay)
        
        return all_data
    
    def save_to_csv(self, data, filename='followers.csv'):
        """ä¿å­˜æ•°æ®åˆ°CSVæ–‡ä»¶"""
        if not data:
            print("âŒ æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
            return
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨,å¦‚æœå­˜åœ¨åˆ™è¯»å–å·²æœ‰æ•°æ®
        try:
            if os.path.exists(filename):
                existing_df = pd.read_csv(filename, encoding='utf-8-sig')
                print(f"\nğŸ“– ä» {filename} è¯»å–å·²æœ‰æ•°æ®...")
            else:
                existing_df = pd.DataFrame(columns=['æ—¥æœŸ', 'è´¦å·å', 'å¹³å°', 'ç²‰ä¸æ•°'])
                print(f"\nğŸ“ åˆ›å»ºæ–°çš„æ•°æ®æ–‡ä»¶ {filename}")
        except Exception as e:
            print(f"âš ï¸ è¯»å–å·²æœ‰æ•°æ®æ—¶å‡ºé”™: {str(e)}")
            existing_df = pd.DataFrame(columns=['æ—¥æœŸ', 'è´¦å·å', 'å¹³å°', 'ç²‰ä¸æ•°'])

        # å°†æ–°æ•°æ®è½¬æ¢ä¸ºDataFrame
        df = pd.DataFrame(data)
        output_columns = ['æ—¥æœŸ', 'è´¦å·å', 'å¹³å°', 'ç²‰ä¸æ•°']
        df_output = df[output_columns]
        
        # åˆå¹¶æ–°æ—§æ•°æ®
        combined_df = pd.concat([existing_df, df_output], ignore_index=True)
        
        # ä¿å­˜åˆå¹¶åçš„æ•°æ®
        combined_df.to_csv(filename, index=False, encoding='utf-8-sig')
        print(f"\nâœ… æ•°æ®å·²è¿½åŠ ä¿å­˜åˆ° {filename}")
        
        # ç»Ÿè®¡ä¿¡æ¯
        successful_data = [item for item in data if item['ç²‰ä¸æ•°'] > 0]
        print(f"\nğŸ“Š æœ¬æ¬¡ç»Ÿè®¡ä¿¡æ¯:")
        print(f"   æ€»è´¦å·æ•°: {len(data)}")
        print(f"   æˆåŠŸè·å–: {len(successful_data)}")
        print(f"   å¤±è´¥æ•°é‡: {len(data) - len(successful_data)}")
        
        if successful_data:
            total_fans = sum(item['ç²‰ä¸æ•°'] for item in successful_data)
            avg_fans = total_fans // len(successful_data) if successful_data else 0
            print(f"   æ€»ç²‰ä¸æ•°: {total_fans:,}")
            print(f"   å¹³å‡ç²‰ä¸æ•°: {avg_fans:,}")
        
        print(f"\nğŸ“‹ æœ€æ–°æ•°æ®é¢„è§ˆ:")
        print(df_output.head().to_string(index=False))

# åœ¨æ–‡ä»¶å¼€å¤´çš„å¯¼å…¥éƒ¨åˆ†åæ·»åŠ cookieè¯»å–å‡½æ•°
def load_cookie_from_json(cookie_file='douyin_cookie.json'):
    """ä»JSONæ–‡ä»¶ä¸­è¯»å–cookieå¹¶è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼"""
    try:
        if not os.path.exists(cookie_file):
            print(f"âŒ æ‰¾ä¸åˆ°cookieæ–‡ä»¶: {cookie_file}")
            return ""
            
        with open(cookie_file, 'r', encoding='utf-8') as f:
            cookies_data = json.load(f)
        
        # è½¬æ¢ä¸ºcookieå­—ç¬¦ä¸²
        cookie_pairs = []
        for cookie in cookies_data:
            name = cookie.get('name', '')
            value = cookie.get('value', '')
            if name and value:
                cookie_pairs.append(f"{name}={value}")
        
        cookie_string = '; '.join(cookie_pairs)
        print(f"âœ… æˆåŠŸä» {cookie_file} è¯»å–cookieä¿¡æ¯")
        return cookie_string
        
    except Exception as e:
        print(f"âŒ è¯»å–cookieæ–‡ä»¶å¤±è´¥: {str(e)}")
        return ""

async def main():
    # ä»JSONæ–‡ä»¶è¯»å–cookie
    cookie = load_cookie_from_json('douyin_cookie.json')
    
    if not cookie:
        print("âŒ æ— æ³•è·å–cookieä¿¡æ¯ï¼Œç¨‹åºé€€å‡º")
        return
    
    user_list = ["357368605", "superslow"]
    
    collector = DouyinFansCollectorEnhanced(cookie=cookie)
    
    print("ğŸš€ å¼€å§‹æ”¶é›†æŠ–éŸ³ç²‰ä¸æ•°æ®ï¼ˆå¢å¼ºç‰ˆï¼‰...")
    print(f"ğŸ“‹ å¾…å¤„ç†æŠ–éŸ³å·: {', '.join(user_list)}")
    
    fans_data = await collector.collect_fans_data(user_list)
    collector.save_to_csv(fans_data, 'followers.csv')
    
    print("\nğŸ‰ æ•°æ®æ”¶é›†å®Œæˆï¼")

if __name__ == "__main__":
    os.environ['PYTHONIOENCODING'] = 'utf-8'
    asyncio.run(main())